{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "deec0bd6-0e72-4892-99bc-92ff44983e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c036b1d-6007-40ac-b018-301677db8c9d",
   "metadata": {},
   "source": [
    "#### Iniciando a sessao Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7bbe17-1fca-46f6-b67a-afb8bf8882ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"CreateABT\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ce4298-969e-4a27-ae82-93c1d9f35759",
   "metadata": {},
   "source": [
    "## Leitura das tabelas e tratamento dos dados\n",
    "Obejtivo desta etapa é pegar os dados baixados do banco de dados e tratar os dados, apagando os dados duplicados, juntar as tabelas e criar mais variaveis explicativas para enriquecer os dados para os modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9edc5958-13d2-4821-87d6-faea5883c52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_daumau = \"daumau.csv\"\n",
    "path_desinstalacoes = \"desinstalacoes.csv\"\n",
    "path_instalacoes = \"installs.csv\"\n",
    "path_ratings = \"ratings_reviews.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ed29c57b-1bcd-4f4b-850a-c0f00dbeab56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47155"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daumau_00 = spark.read.csv(path_daumau, inferSchema = True, header = True)\n",
    "daumau_00.createOrReplaceTempView(\"daumau_00\")\n",
    "daumau_00.cache()\n",
    "daumau_00.count()\n",
    "\n",
    "\n",
    "desinstalacoes_00 = spark.read.csv(path_desinstalacoes, inferSchema = True, header = True)\n",
    "desinstalacoes_00.createOrReplaceTempView(\"desinstalacoes_00\")\n",
    "desinstalacoes_00.cache()\n",
    "desinstalacoes_00.count()\n",
    "\n",
    "\n",
    "instalacoes_00 = spark.read.csv(path_instalacoes, inferSchema = True, header = True)\n",
    "instalacoes_00.createOrReplaceTempView(\"instalacoes_00\")\n",
    "instalacoes_00.cache()\n",
    "instalacoes_00.count()\n",
    "\n",
    "\n",
    "ratings_00 = spark.read.csv(path_ratings, inferSchema = True, header = True)\n",
    "ratings_00.createOrReplaceTempView(\"ratings_00\")\n",
    "ratings_00.cache()\n",
    "ratings_00.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439f30c7-2330-49db-ad84-246ac3d2df82",
   "metadata": {},
   "source": [
    "#### Essas duas tabelas havia dados duplicados, utilizei o distinct para remover os dados duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b35f0102-c773-45ba-a42a-ddbe332e7d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41146"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instalacoes_01 = instalacoes_00.distinct()\n",
    "instalacoes_01.createOrReplaceTempView(\"instalacoes_01\")\n",
    "instalacoes_01.cache()\n",
    "instalacoes_01.count()\n",
    "\n",
    "\n",
    "daumau_01 = daumau_00.distinct()\n",
    "daumau_01.createOrReplaceTempView(\"daumau_01\")\n",
    "daumau_01.cache()\n",
    "daumau_01.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9845c49f-d88e-43f6-9666-e607efcdf394",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47155"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abt_00 = spark.sql(\"\"\"\n",
    "    select\n",
    "        a.date, \n",
    "        a.appid, \n",
    "        a.category, \n",
    "        a.ratings, \n",
    "        a.daily_ratings, \n",
    "        a.reviews, \n",
    "        a.daily_reviews,\n",
    "        b.newinstalls\n",
    "    from \n",
    "        ratings_00 a\n",
    "    left join\n",
    "        instalacoes_01 b\n",
    "    on (a.appid = b.appid and a.date = b.date)\n",
    "    \n",
    "\n",
    "\n",
    "\"\"\")\n",
    "abt_00.createOrReplaceTempView(\"abt_00\")\n",
    "abt_00.cache()\n",
    "abt_00.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a14da3-f451-47e5-a19d-00ffddf56756",
   "metadata": {},
   "source": [
    "#### Criei uma variavel chamada diff_install_uninstall que basicamente é a diferença entre as intalações e as desinstalações do aplicativo numa data especifica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c5245df7-87de-40a8-a084-4cea8690f390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47155"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abt_01 = spark.sql(\"\"\"\n",
    "    select\n",
    "        a.*,\n",
    "        b.predictionLoss ,\n",
    "        a.newinstalls - b.predictionLoss as diff_install_uninstall\n",
    "    from\n",
    "        abt_00 a\n",
    "    left join\n",
    "        desinstalacoes_00 b\n",
    "    on (a.appid = b.appId and a.date = b.date )\n",
    "\n",
    "        \"\"\")\n",
    "abt_01.createOrReplaceTempView(\"abt_01\")\n",
    "abt_01.cache()\n",
    "abt_01.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "69e17ece-eb30-4325-8c40-3dcced9bd410",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47155"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abt_02 = spark.sql(\"\"\"\n",
    "\n",
    "    select\n",
    "        a.*,\n",
    "        b.dauReal\n",
    "    from\n",
    "        abt_01 a\n",
    "    left join \n",
    "        daumau_01 b\n",
    "    on (a.appid = b.appId and a.date = b.date)\n",
    "\n",
    "            \"\"\")\n",
    "abt_02.createOrReplaceTempView(\"abt_02\")\n",
    "abt_02.cache()\n",
    "abt_02.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f0c10a-7d92-4b37-a17a-c898207213da",
   "metadata": {},
   "source": [
    "#### Neste processo eu desenvolvo um pequeno book de variaveis que consiste em criar algumas variaveis utilizando diferentes combinações matemáticas em diferentes janelas de tempo (15, 30, 45, 60, 90 dias). Essa é uma tecnica muito utilizada em bancos para aproveitar 100% do potencial dos dados do Data Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "5cf65070-f0cf-41d7-8599-3b9258aa1cbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47155"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abt_03 = spark.sql(\"\"\"\n",
    "\n",
    "    select\n",
    "        a.date, \n",
    "        a.appid, \n",
    "        a.category, \n",
    "       case when category = 'FINANCE' then 1\n",
    "            when category = 'OTHERS' then 2\n",
    "            when category = 'SHOPPING' then 3\n",
    "            when category = 'BUSINESS' then 4\n",
    "            when category = 'TRAVEL_AND_LOCAL' then 5\n",
    "            when category = 'FOOD_AND_DRINK' then 6\n",
    "            end as category_flag,\n",
    "        \n",
    "        a.ratings, \n",
    "        a.daily_ratings, \n",
    "        a.reviews, \n",
    "        a.daily_reviews, \n",
    "        a.newinstalls, \n",
    "        a.predictionLoss, \n",
    "    \n",
    "        \n",
    "        sum(daily_ratings) over (partition by appid order by date rows between 15 preceding and current row) AS sum_15_daily_ratings,\n",
    "        avg(daily_ratings) over (partition by appid order by date rows between 15 preceding and current row) AS avg_15_daily_ratings,\n",
    "        min(daily_ratings) over (partition by appid order by date rows between 15 preceding and current row) AS min_15_daily_ratings,\n",
    "        max(daily_ratings) over (partition by appid order by date rows between 15 preceding and current row) AS max_15_daily_ratings,\n",
    "        sum(daily_ratings) over (partition by appid order by date rows between 30 preceding and current row) AS sum_30_daily_ratings,\n",
    "        avg(daily_ratings) over (partition by appid order by date rows between 30 preceding and current row) AS avg_30_daily_ratings,\n",
    "        min(daily_ratings) over (partition by appid order by date rows between 30 preceding and current row) AS min_30_daily_ratings,\n",
    "        max(daily_ratings) over (partition by appid order by date rows between 30 preceding and current row) AS max_30_daily_ratings,\n",
    "        sum(daily_ratings) over (partition by appid order by date rows between 45 preceding and current row) AS sum_45_daily_ratings,\n",
    "        avg(daily_ratings) over (partition by appid order by date rows between 45 preceding and current row) AS avg_45_daily_ratings,\n",
    "        min(daily_ratings) over (partition by appid order by date rows between 45 preceding and current row) AS min_45_daily_ratings,\n",
    "        max(daily_ratings) over (partition by appid order by date rows between 45 preceding and current row) AS max_45_daily_ratings,\n",
    "        sum(daily_ratings) over (partition by appid order by date rows between 60 preceding and current row) AS sum_60_daily_ratings,\n",
    "        avg(daily_ratings) over (partition by appid order by date rows between 60 preceding and current row) AS avg_60_daily_ratings,\n",
    "        min(daily_ratings) over (partition by appid order by date rows between 60 preceding and current row) AS min_60_daily_ratings,\n",
    "        max(daily_ratings) over (partition by appid order by date rows between 60 preceding and current row) AS max_60_daily_ratings,\n",
    "        sum(daily_ratings) over (partition by appid order by date rows between 90 preceding and current row) AS sum_90_daily_ratings,\n",
    "        avg(daily_ratings) over (partition by appid order by date rows between 90 preceding and current row) AS avg_90_daily_ratings,\n",
    "        min(daily_ratings) over (partition by appid order by date rows between 90 preceding and current row) AS min_90_daily_ratings,\n",
    "        max(daily_ratings) over (partition by appid order by date rows between 90 preceding and current row) AS max_90_daily_ratings,\n",
    "        sum(daily_reviews) over (partition by appid order by date rows between 15 preceding and current row) AS sum_15_daily_reviews,\n",
    "        avg(daily_reviews) over (partition by appid order by date rows between 15 preceding and current row) AS avg_15_daily_reviews,\n",
    "        min(daily_reviews) over (partition by appid order by date rows between 15 preceding and current row) AS min_15_daily_reviews,\n",
    "        max(daily_reviews) over (partition by appid order by date rows between 15 preceding and current row) AS max_15_daily_reviews,\n",
    "        sum(daily_reviews) over (partition by appid order by date rows between 30 preceding and current row) AS sum_30_daily_reviews,\n",
    "        avg(daily_reviews) over (partition by appid order by date rows between 30 preceding and current row) AS avg_30_daily_reviews,\n",
    "        min(daily_reviews) over (partition by appid order by date rows between 30 preceding and current row) AS min_30_daily_reviews,\n",
    "        max(daily_reviews) over (partition by appid order by date rows between 30 preceding and current row) AS max_30_daily_reviews,\n",
    "        sum(daily_reviews) over (partition by appid order by date rows between 45 preceding and current row) AS sum_45_daily_reviews,\n",
    "        avg(daily_reviews) over (partition by appid order by date rows between 45 preceding and current row) AS avg_45_daily_reviews,\n",
    "        min(daily_reviews) over (partition by appid order by date rows between 45 preceding and current row) AS min_45_daily_reviews,\n",
    "        max(daily_reviews) over (partition by appid order by date rows between 45 preceding and current row) AS max_45_daily_reviews,\n",
    "        sum(daily_reviews) over (partition by appid order by date rows between 60 preceding and current row) AS sum_60_daily_reviews,\n",
    "        avg(daily_reviews) over (partition by appid order by date rows between 60 preceding and current row) AS avg_60_daily_reviews,\n",
    "        min(daily_reviews) over (partition by appid order by date rows between 60 preceding and current row) AS min_60_daily_reviews,\n",
    "        max(daily_reviews) over (partition by appid order by date rows between 60 preceding and current row) AS max_60_daily_reviews,\n",
    "        sum(daily_reviews) over (partition by appid order by date rows between 90 preceding and current row) AS sum_90_daily_reviews,\n",
    "        avg(daily_reviews) over (partition by appid order by date rows between 90 preceding and current row) AS avg_90_daily_reviews,\n",
    "        min(daily_reviews) over (partition by appid order by date rows between 90 preceding and current row) AS min_90_daily_reviews,\n",
    "        max(daily_reviews) over (partition by appid order by date rows between 90 preceding and current row) AS max_90_daily_reviews,\n",
    "        sum(newinstalls) over (partition by appid order by date rows between 15 preceding and current row) AS sum_15_newinstalls,\n",
    "        avg(newinstalls) over (partition by appid order by date rows between 15 preceding and current row) AS avg_15_newinstalls,\n",
    "        min(newinstalls) over (partition by appid order by date rows between 15 preceding and current row) AS min_15_newinstalls,\n",
    "        max(newinstalls) over (partition by appid order by date rows between 15 preceding and current row) AS max_15_newinstalls,\n",
    "        sum(newinstalls) over (partition by appid order by date rows between 30 preceding and current row) AS sum_30_newinstalls,\n",
    "        avg(newinstalls) over (partition by appid order by date rows between 30 preceding and current row) AS avg_30_newinstalls,\n",
    "        min(newinstalls) over (partition by appid order by date rows between 30 preceding and current row) AS min_30_newinstalls,\n",
    "        max(newinstalls) over (partition by appid order by date rows between 30 preceding and current row) AS max_30_newinstalls,\n",
    "        sum(newinstalls) over (partition by appid order by date rows between 45 preceding and current row) AS sum_45_newinstalls,\n",
    "        avg(newinstalls) over (partition by appid order by date rows between 45 preceding and current row) AS avg_45_newinstalls,\n",
    "        min(newinstalls) over (partition by appid order by date rows between 45 preceding and current row) AS min_45_newinstalls,\n",
    "        max(newinstalls) over (partition by appid order by date rows between 45 preceding and current row) AS max_45_newinstalls,\n",
    "        sum(newinstalls) over (partition by appid order by date rows between 60 preceding and current row) AS sum_60_newinstalls,\n",
    "        avg(newinstalls) over (partition by appid order by date rows between 60 preceding and current row) AS avg_60_newinstalls,\n",
    "        min(newinstalls) over (partition by appid order by date rows between 60 preceding and current row) AS min_60_newinstalls,\n",
    "        max(newinstalls) over (partition by appid order by date rows between 60 preceding and current row) AS max_60_newinstalls,\n",
    "        sum(newinstalls) over (partition by appid order by date rows between 90 preceding and current row) AS sum_90_newinstalls,\n",
    "        avg(newinstalls) over (partition by appid order by date rows between 90 preceding and current row) AS avg_90_newinstalls,\n",
    "        min(newinstalls) over (partition by appid order by date rows between 90 preceding and current row) AS min_90_newinstalls,\n",
    "        max(newinstalls) over (partition by appid order by date rows between 90 preceding and current row) AS max_90_newinstalls,\n",
    "        sum(predictionLoss) over (partition by appid order by date rows between 15 preceding and current row) AS sum_15_predictionLoss,\n",
    "        avg(predictionLoss) over (partition by appid order by date rows between 15 preceding and current row) AS avg_15_predictionLoss,\n",
    "        min(predictionLoss) over (partition by appid order by date rows between 15 preceding and current row) AS min_15_predictionLoss,\n",
    "        max(predictionLoss) over (partition by appid order by date rows between 15 preceding and current row) AS max_15_predictionLoss,\n",
    "        sum(predictionLoss) over (partition by appid order by date rows between 30 preceding and current row) AS sum_30_predictionLoss,\n",
    "        avg(predictionLoss) over (partition by appid order by date rows between 30 preceding and current row) AS avg_30_predictionLoss,\n",
    "        min(predictionLoss) over (partition by appid order by date rows between 30 preceding and current row) AS min_30_predictionLoss,\n",
    "        max(predictionLoss) over (partition by appid order by date rows between 30 preceding and current row) AS max_30_predictionLoss,\n",
    "        sum(predictionLoss) over (partition by appid order by date rows between 45 preceding and current row) AS sum_45_predictionLoss,\n",
    "        avg(predictionLoss) over (partition by appid order by date rows between 45 preceding and current row) AS avg_45_predictionLoss,\n",
    "        min(predictionLoss) over (partition by appid order by date rows between 45 preceding and current row) AS min_45_predictionLoss,\n",
    "        max(predictionLoss) over (partition by appid order by date rows between 45 preceding and current row) AS max_45_predictionLoss,\n",
    "        sum(predictionLoss) over (partition by appid order by date rows between 60 preceding and current row) AS sum_60_predictionLoss,\n",
    "        avg(predictionLoss) over (partition by appid order by date rows between 60 preceding and current row) AS avg_60_predictionLoss,\n",
    "        min(predictionLoss) over (partition by appid order by date rows between 60 preceding and current row) AS min_60_predictionLoss,\n",
    "        max(predictionLoss) over (partition by appid order by date rows between 60 preceding and current row) AS max_60_predictionLoss,\n",
    "        sum(predictionLoss) over (partition by appid order by date rows between 90 preceding and current row) AS sum_90_predictionLoss,\n",
    "        avg(predictionLoss) over (partition by appid order by date rows between 90 preceding and current row) AS avg_90_predictionLoss,\n",
    "        min(predictionLoss) over (partition by appid order by date rows between 90 preceding and current row) AS min_90_predictionLoss,\n",
    "        max(predictionLoss) over (partition by appid order by date rows between 90 preceding and current row) AS max_90_predictionLoss,\n",
    "        sum(diff_install_uninstall) over (partition by appid order by date rows between 15 preceding and current row) AS sum_15_diff_install_uninstall,\n",
    "        avg(diff_install_uninstall) over (partition by appid order by date rows between 15 preceding and current row) AS avg_15_diff_install_uninstall,\n",
    "        min(diff_install_uninstall) over (partition by appid order by date rows between 15 preceding and current row) AS min_15_diff_install_uninstall,\n",
    "        max(diff_install_uninstall) over (partition by appid order by date rows between 15 preceding and current row) AS max_15_diff_install_uninstall,\n",
    "        sum(diff_install_uninstall) over (partition by appid order by date rows between 30 preceding and current row) AS sum_30_diff_install_uninstall,\n",
    "        avg(diff_install_uninstall) over (partition by appid order by date rows between 30 preceding and current row) AS avg_30_diff_install_uninstall,\n",
    "        min(diff_install_uninstall) over (partition by appid order by date rows between 30 preceding and current row) AS min_30_diff_install_uninstall,\n",
    "        max(diff_install_uninstall) over (partition by appid order by date rows between 30 preceding and current row) AS max_30_diff_install_uninstall,\n",
    "        sum(diff_install_uninstall) over (partition by appid order by date rows between 45 preceding and current row) AS sum_45_diff_install_uninstall,\n",
    "        avg(diff_install_uninstall) over (partition by appid order by date rows between 45 preceding and current row) AS avg_45_diff_install_uninstall,\n",
    "        min(diff_install_uninstall) over (partition by appid order by date rows between 45 preceding and current row) AS min_45_diff_install_uninstall,\n",
    "        max(diff_install_uninstall) over (partition by appid order by date rows between 45 preceding and current row) AS max_45_diff_install_uninstall,\n",
    "        sum(diff_install_uninstall) over (partition by appid order by date rows between 60 preceding and current row) AS sum_60_diff_install_uninstall,\n",
    "        avg(diff_install_uninstall) over (partition by appid order by date rows between 60 preceding and current row) AS avg_60_diff_install_uninstall,\n",
    "        min(diff_install_uninstall) over (partition by appid order by date rows between 60 preceding and current row) AS min_60_diff_install_uninstall,\n",
    "        max(diff_install_uninstall) over (partition by appid order by date rows between 60 preceding and current row) AS max_60_diff_install_uninstall,\n",
    "        sum(diff_install_uninstall) over (partition by appid order by date rows between 90 preceding and current row) AS sum_90_diff_install_uninstall,\n",
    "        avg(diff_install_uninstall) over (partition by appid order by date rows between 90 preceding and current row) AS avg_90_diff_install_uninstall,\n",
    "        min(diff_install_uninstall) over (partition by appid order by date rows between 90 preceding and current row) AS min_90_diff_install_uninstall,\n",
    "        max(diff_install_uninstall) over (partition by appid order by date rows between 90 preceding and current row) AS max_90_diff_install_uninstall, \n",
    "\n",
    "        a.dauReal\n",
    "        from abt_02 a\n",
    "        \n",
    "\n",
    "\"\"\")\n",
    "abt_03.createOrReplaceTempView(\"abt_03\")\n",
    "abt_03.cache()\n",
    "abt_03.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02cc7cd-4350-44ff-a41d-270da479f0c9",
   "metadata": {},
   "source": [
    "#### Removi os dados que tinha o valor do target como nulo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "e706e604-3a5d-4477-84e3-47c98a045b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "abt_04 = abt_03.where(\"dauReal is not null\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13054143-e4d2-4b4a-9b4f-e082428835b0",
   "metadata": {},
   "source": [
    "#### Salvei os dados em csv dentro da pasta abt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "3fc8d926-a7ae-42d8-a50d-bd45c74ec480",
   "metadata": {},
   "outputs": [],
   "source": [
    "abt_04.repartition(1).write.csv(\"abt\", header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe1c1bb-bfcc-45f4-acdc-387b3b27d6b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e856012-7f3c-4a67-b570-48d3cca82be9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
